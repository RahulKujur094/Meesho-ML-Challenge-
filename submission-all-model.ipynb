{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84705,"databundleVersionId":9755748,"sourceType":"competition"},{"sourceId":9741905,"sourceType":"datasetVersion","datasetId":5963174},{"sourceId":9818987,"sourceType":"datasetVersion","datasetId":6020298},{"sourceId":9825967,"sourceType":"datasetVersion","datasetId":6025689},{"sourceId":9911943,"sourceType":"datasetVersion","datasetId":6090380},{"sourceId":9911935,"sourceType":"datasetVersion","datasetId":6090372},{"sourceId":9911937,"sourceType":"datasetVersion","datasetId":6090374},{"sourceId":9911932,"sourceType":"datasetVersion","datasetId":6090369}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-15T05:47:31.515671Z","iopub.execute_input":"2024-11-15T05:47:31.516348Z","iopub.status.idle":"2024-11-15T05:47:31.520880Z","shell.execute_reply.started":"2024-11-15T05:47:31.516311Z","shell.execute_reply":"2024-11-15T05:47:31.520000Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\n\ndef organize_images_by_category(train_csv_path, images_folder_path, output_base_path):\n    # Load the CSV file\n    df = pd.read_csv(train_csv_path)\n\n    # Create a dictionary to map categories to folder names\n    categories = {\n        \"Women Tops & Tunics\": \"Women_Tops_&_Tunics\",\n        \"Women Tshirts\": \"Women_Tshirts\",\n        \"Sarees\": \"Sarees\",\n        \"Men Tshirts\": \"Men_Tshirts\",\n        \"Kurtis\": \"Kurtis\"\n    }\n\n    # Create folders for each category in /kaggle/working\n    for category in categories.values():\n        folder_path = os.path.join(output_base_path, category)\n        os.makedirs(folder_path, exist_ok=True)\n\n    # Iterate over the rows in the CSV and copy images to corresponding category folders\n    for _, row in df.iterrows():\n        image_id = str(row['id']).zfill(6)  # Convert ID to 6-digit string with leading zeros\n        image_name = f\"{image_id}.jpg\"\n        category = row['Category']\n\n        if category in categories:\n            # Get the source image path\n            src_image_path = os.path.join(images_folder_path, image_name)\n            \n            # Get the destination folder for the category\n            dest_folder = os.path.join(output_base_path, categories[category])\n            \n            # Get the destination image path\n            dest_image_path = os.path.join(dest_folder, image_name)\n\n            # Copy the image to the destination folder\n            if os.path.exists(src_image_path):\n                shutil.copy(src_image_path, dest_image_path)  # Copy instead of move\n                #print(f\"Copied {image_name} to {categories[category]} folder.\")\n            else:\n                print(f\"Image {image_name} not found in {images_folder_path}.\")\n\nif __name__ == \"__main__\":\n    # Path to the CSV file and images folder\n    train_csv_path = \"/kaggle/input/visual-taxonomy/test.csv\"  # Update with actual path to train.csv\n    images_folder_path = \"/kaggle/input/visual-taxonomy/test_images\"  # Update with actual path to train_images\n    output_base_path = \"/kaggle/working/sorted_images\"  # Writable directory\n\n    # Organize the images by category\n    organize_images_by_category(train_csv_path, images_folder_path, output_base_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-06T03:30:14.504024Z","iopub.execute_input":"2024-11-06T03:30:14.504570Z","iopub.status.idle":"2024-11-06T03:33:24.352581Z","shell.execute_reply.started":"2024-11-06T03:30:14.504530Z","shell.execute_reply":"2024-11-06T03:33:24.351710Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef plot_image_if_exists(image_folder, image_name):\n    # Construct the full path to the image\n    image_path = os.path.join(image_folder, image_name)\n    \n    # Check if the image exists\n    if os.path.exists(image_path):\n        # Load the image\n        image = Image.open(image_path)\n        \n        # Plot the image\n        plt.imshow(image)\n        plt.axis('off')  # Hide axis\n        plt.title(f\"Image: {image_name}\")  # Set title\n        plt.show()  # Display the image\n    else:\n        print(f\"Image '{image_name}' does not exist in the folder '{image_folder}'.\")\n\n# Example usage\nimage_folder = \"/kaggle/working/sorted_images/Men_Tshirts\"  # Replace with your folder path\nimage_name = \"/kaggle/input/visual-taxonomy/test_images/000001.jpg\"  # Replace with the image name you want to check\n\nplot_image_if_exists(image_folder, image_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:15:22.749411Z","iopub.execute_input":"2024-10-23T18:15:22.750281Z","iopub.status.idle":"2024-10-23T18:15:23.087197Z","shell.execute_reply.started":"2024-10-23T18:15:22.75024Z","shell.execute_reply":"2024-10-23T18:15:23.086278Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load your model (replace 'your_model.h5' with the actual model path)\nmodel = load_model('/kaggle/input/modelm/model_checkpoint.keras')\n\n# Verify that the model has been loaded\nprint(\"Model loaded successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:29:42.35133Z","iopub.execute_input":"2024-10-23T18:29:42.352531Z","iopub.status.idle":"2024-10-23T18:29:45.190345Z","shell.execute_reply.started":"2024-10-23T18:29:42.352487Z","shell.execute_reply":"2024-10-23T18:29:45.189419Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.saving import register_keras_serializable\n\n@register_keras_serializable()\ndef custom_loss_function(y_true, y_pred):\n    \"\"\"\n    Custom loss function to ignore the last class (dummy) during loss calculation.\n    \"\"\"\n    # Exclude the last class (dummy) from the loss calculation\n    y_true = tf.cast(y_true[:, :-1], tf.float32)  # Exclude last class\n    y_pred = y_pred[:, :-1]  # Exclude last class\n    loss = tf.keras.backend.categorical_crossentropy(y_true, y_pred)  # Use tf.keras.backend\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T05:47:33.870360Z","iopub.execute_input":"2024-11-15T05:47:33.870722Z","iopub.status.idle":"2024-11-15T05:47:45.913989Z","shell.execute_reply.started":"2024-11-15T05:47:33.870687Z","shell.execute_reply":"2024-11-15T05:47:45.913227Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load your model (replace 'your_model.h5' with the actual model path)\nmodel = load_model('/kaggle/input/model-bettercorr-s/model_checkpoint.keras')\n#model2 = load_model('/kaggle/input/model-ensemble-m/model_checkpoint.keras')\n#model3 = load_model('/kaggle/input/model-inception-m/model_checkpoint.keras')\n\n\n# Verify that the model has been loaded\nprint(\"Model loaded successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T05:49:32.682933Z","iopub.execute_input":"2024-11-15T05:49:32.683322Z","iopub.status.idle":"2024-11-15T05:50:03.404586Z","shell.execute_reply.started":"2024-11-15T05:49:32.683278Z","shell.execute_reply":"2024-11-15T05:50:03.403614Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model loaded successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the model summary to see its architecture\nmodel.summary()\n\n# Check the input shape of the model\ninput_shape = model.input_shape\nprint(f\"Expected input shape: {input_shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:30:19.258375Z","iopub.execute_input":"2024-10-23T18:30:19.258795Z","iopub.status.idle":"2024-10-23T18:30:19.53634Z","shell.execute_reply.started":"2024-10-23T18:30:19.258755Z","shell.execute_reply":"2024-10-23T18:30:19.535474Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\n\ndef preprocess_image(image_path):\n    # Load the image\n    image = Image.open(image_path)\n    \n    # Resize the image to (198, 198)\n    image = image.resize((198, 198))\n    \n    # Convert the image to a numpy array and normalize pixel values\n    image_array = np.array(image) / 255.0\n    \n    return image_array\n\n# Example usage\nimage_path = '/kaggle/input/visual-taxonomy/test_images/000000.jpg'  # Replace with your image path\nprocessed_image = preprocess_image(image_path)\nprint(processed_image.shape)  # Should be (198, 198, 3) for RGB images\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:16:18.572367Z","iopub.execute_input":"2024-10-23T18:16:18.572781Z","iopub.status.idle":"2024-10-23T18:16:18.58804Z","shell.execute_reply.started":"2024-10-23T18:16:18.572742Z","shell.execute_reply":"2024-10-23T18:16:18.58711Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import load_model\nfrom PIL import Image\n\ndef preprocess_image(image_path):\n    # Load the image\n    image = Image.open(image_path)\n    \n    # Resize the image to (198, 198)\n    image = image.resize((198, 198))\n    \n    # Convert the image to a numpy array and normalize pixel values\n    image_array = np.array(image) / 255.0\n    \n    # Add a batch dimension (1, 198, 198, 3)\n    image_array = np.expand_dims(image_array, axis=0)\n    \n    return image_array\n\n# Load your model\nmodel = load_model('/kaggle/input/modelm/model_checkpoint.keras')  # Replace with your model path\n\n# Predict on a new image\nimage_path = '/kaggle/input/visual-taxonomy/test_images/000000.jpg'  # Replace with your image path\nprocessed_image = preprocess_image(image_path)\n\n# Make predictions\npredictions = model.predict(processed_image)\n\n# Print the raw predictions\nprint(f\"Raw predictions for {image_path}: {predictions}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:16:31.56807Z","iopub.execute_input":"2024-10-23T18:16:31.568918Z","iopub.status.idle":"2024-10-23T18:16:37.84255Z","shell.execute_reply.started":"2024-10-23T18:16:31.568853Z","shell.execute_reply":"2024-10-23T18:16:37.841578Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.preprocessing.image import load_img, img_to_array\n\n# Load and preprocess the image\ndef preprocess_image(image_path):\n    # Load the image with target size (198, 198)\n    img = load_img(image_path, target_size=(198, 198))\n    img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n    return np.expand_dims(img_array, axis=0)  # Add batch dimension\n\n# Load your model (assuming model is already defined and compiled)\n# model = ... # Your model loading code here\n\n# Example image path\nimage_path = \"/kaggle/input/visual-taxonomy/test_images/000000.jpg\"\n\n# Preprocess the image\npreprocessed_image = preprocess_image(image_path)\n\n# Get raw predictions from the model\nraw_predictions = model.predict(preprocessed_image)\n\n# Print raw predictions\nprint(f\"Raw predictions for {image_path}: {raw_predictions}\")\n\n# Apply argmax to get predicted class labels\npredicted_classes = [np.argmax(pred, axis=-1) for pred in raw_predictions]\n\n# Print the predicted class labels for each attribute\nprint(\"Predicted classes:\")\nfor idx, pred in enumerate(predicted_classes):\n    print(f\"Attribute {idx + 1}: {pred}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:16:51.971207Z","iopub.execute_input":"2024-10-23T18:16:51.972153Z","iopub.status.idle":"2024-10-23T18:16:52.054468Z","shell.execute_reply.started":"2024-10-23T18:16:51.9721Z","shell.execute_reply":"2024-10-23T18:16:52.053585Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your attribute mappings\nattribute_mapping = {\n    'attr_1': {0: 'black', 1: 'default', 2: 'multicolor', 3: 'white', 4: 'dummy'},\n    'attr_2': {0: 'polo', 1: 'round', 2: 'dummy'},\n    'attr_3': {0: 'printed', 1: 'solid', 2: 'dummy'},\n    'attr_4': {0: 'default', 1: 'solid', 2: 'typography', 3: 'dummy'},\n    'attr_5': {0: 'long sleeves', 1: 'short sleeves', 2: 'dummy'}\n}\n\n# Map predicted indices to labels\npredicted_labels = {f'attr_{i + 1}': attribute_mapping[f'attr_{i + 1}'].get(pred[0], 'dummy') for i, pred in enumerate(predicted_classes)}\n\n# Print the predicted labels\nprint(\"Predicted labels:\")\nfor attr, label in predicted_labels.items():\n    print(f\"{attr}: {label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:16:59.986114Z","iopub.execute_input":"2024-10-23T18:16:59.986493Z","iopub.status.idle":"2024-10-23T18:16:59.994435Z","shell.execute_reply.started":"2024-10-23T18:16:59.986457Z","shell.execute_reply":"2024-10-23T18:16:59.993544Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function: Resize images to (198, 198) and normalize pixel values to [0, 1]\ndef preprocess_image(image_path):\n    img = image.load_img(image_path, target_size=(198, 198))  # Resize to model's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0                             # Normalize pixel values\n    return img_array\n\n# Attribute mapping dictionary provided\nattribute_mapping = {\n    'attr_1': {0: 'black', 1: 'default', 2: 'multicolor', 3: 'white', 4: 'dummy'},\n    'attr_2': {0: 'polo', 1: 'round', 2: 'dummy'},\n    'attr_3': {0: 'printed', 1: 'solid', 2: 'dummy'},\n    'attr_4': {0: 'default', 1: 'solid', 2: 'typography', 3: 'dummy'},\n    'attr_5': {0: 'long sleeves', 1: 'short sleeves', 2: 'dummy'}\n}\n\n# Function to predict attributes for images in batches\ndef predict_attributes(image_folder, model, batch_size=128):\n    predictions = []\n    \n    # Get all image files and sort them numerically based on the file name\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sorting by numerical part of the file name\n\n    for i in range(0, len(image_files), batch_size):\n        batch_files = image_files[i:i + batch_size]  # Process batch by batch\n        batch_images = np.array([preprocess_image(os.path.join(image_folder, file)) for file in batch_files])\n\n        # Predict attributes for the batch\n        raw_predictions = model.predict(batch_images)\n\n        # Extract predicted classes for each attribute\n        predicted_classes = [np.argmax(pred, axis=-1) for pred in raw_predictions]\n\n        # Map predicted indices to attribute labels\n        for idx, file in enumerate(batch_files):\n            predicted_labels = {\n                f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]] \n                for j in range(len(predicted_classes))\n            }\n            #print(f\"Image: {file}\")\n            #print(\"Predicted labels:\", predicted_labels)\n            #print(\"---\")\n\n    return predictions\n\n# Example usage\nimage_folder = \"/kaggle/working/sorted_images/Men_Tshirts\"  # Update with your folder path\npredict_attributes(image_folder, model, batch_size=128)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T18:17:15.15703Z","iopub.execute_input":"2024-10-23T18:17:15.157405Z","iopub.status.idle":"2024-10-23T18:17:32.814395Z","shell.execute_reply.started":"2024-10-23T18:17:15.157371Z","shell.execute_reply":"2024-10-23T18:17:32.813583Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function: Resize images to (198, 198) and normalize pixel values to [0, 1]\ndef preprocess_image(image_path):\n    img = image.load_img(image_path, target_size=(198, 198))  # Resize to model's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0                             # Normalize pixel values\n    return img_array\n\n# Attribute mapping dictionary\nattribute_mapping = {\n    'attr_1': {0: 'black', 1: 'blue', 2: 'default', 3: 'green', 4: 'maroon', 5: 'multicolor', 6: 'navy blue', 7: 'peach', 8: 'pink', 9: 'red', 10: 'white', 11: 'yellow', 12: 'dummy'}, 'attr_2': {0: 'boxy', 1: 'default', 2: 'fitted', 3: 'regular', 4: 'dummy'}, 'attr_3': {0: 'crop', 1: 'regular', 2: 'dummy'}, 'attr_4': {0: 'default', 1: 'high', 2: 'round neck', 3: 'square neck', 4: 'stylised', 5: 'sweetheart neck', 6: 'v-neck', 7: 'dummy'}, 'attr_5': {0: 'casual', 1: 'party', 2: 'dummy'}, 'attr_6': {0: 'default', 1: 'printed', 2: 'solid', 3: 'dummy'}, 'attr_7': {0: 'default', 1: 'floral', 2: 'graphic', 3: 'quirky', 4: 'solid', 5: 'typography', 6: 'dummy'}, 'attr_8': {0: 'long sleeves', 1: 'short sleeves', 2: 'sleeveless', 3: 'three-quarter sleeves', 4: 'dummy'}, 'attr_9': {0: 'default', 1: 'puff sleeves', 2: 'regular sleeves', 3: 'sleeveless', 4: 'dummy'}, 'attr_10': {0: 'applique', 1: 'default', 2: 'knitted', 3: 'ruffles', 4: 'tie-ups', 5: 'waist tie-ups', 6: 'dummy'}\n}\n\n# Function to predict attributes for images in batches\ndef predict_attributes_and_save_to_csv(image_folder, model, batch_size=128):\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sorting by numerical part of the file name\n\n    # CSV header\n    header = ['id', 'Category', 'len', 'attr_1', 'attr_2', 'attr_3', 'attr_4', 'attr_5', 'attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']\n\n    # Open CSV file\n    with open('predictions_WT.csv', mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=header)\n        writer.writeheader()\n\n        for i in range(0, len(image_files), batch_size):\n            batch_files = image_files[i:i + batch_size]\n            batch_images = np.array([preprocess_image(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Predict attributes for the batch\n            raw_predictions = model.predict(batch_images)\n\n            # Extract predicted classes for each attribute\n            predicted_classes = [np.argmax(pred, axis=-1) for pred in raw_predictions]\n\n            for idx, file in enumerate(batch_files):\n                # Extract image id by removing leading zeros and file extension\n                image_id = str(int(file.split('.')[0])) if file != '000000.jpg' else '0'\n\n                # Map predicted classes to attribute labels\n                predicted_labels = {\n                    f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]]\n                    for j in range(len(predicted_classes))\n                }\n\n                # Prepare a row with predicted labels and other columns\n                row = {\n                    'id': image_id,\n                    'Category': 'Women Tops & Tunics',  # Fixed category value\n                    'len': 10,                  # Fixed length value\n                    'attr_1': predicted_labels['attr_1'],\n                    'attr_2': predicted_labels['attr_2'],\n                    'attr_3': predicted_labels['attr_3'],\n                    'attr_4': predicted_labels['attr_4'],\n                    'attr_5': predicted_labels['attr_5'],\n                    'attr_6': predicted_labels['attr_6'],\n                    'attr_7': predicted_labels['attr_7'],\n                    'attr_8': predicted_labels['attr_8'],\n                    'attr_9': predicted_labels['attr_9'],\n                    'attr_10': predicted_labels['attr_10']\n                }\n\n                # Write the row to the CSV\n                writer.writerow(row)\n\n# Example usage\nimage_folder = \"/kaggle/working/sorted_images/Women_Tops_&_Tunics\"  # Update with your folder path\npredict_attributes_and_save_to_csv(image_folder, model, batch_size=128)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-23T19:08:13.253909Z","iopub.execute_input":"2024-10-23T19:08:13.254528Z","iopub.status.idle":"2024-10-23T19:08:50.371543Z","shell.execute_reply.started":"2024-10-23T19:08:13.254488Z","shell.execute_reply":"2024-10-23T19:08:50.370702Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category = \"Sarees\" \nnum_attribute = 10\nattribute_mapping = {\n'attr_1': {0: 'default', 1: 'same as border', 2: 'same as saree', 3: 'solid', 4: 'dummy'}, 'attr_2': {0: 'default', 1: 'no border', 2: 'solid', 3: 'temple border', 4: 'woven design', 5: 'zari', 6: 'dummy'}, 'attr_3': {0: 'big border', 1: 'no border', 2: 'small border', 3: 'dummy'}, 'attr_4': {0: 'cream', 1: 'default', 2: 'green', 3: 'multicolor', 4: 'navy blue', 5: 'pink', 6: 'white', 7: 'yellow', 8: 'dummy'}, 'attr_5': {0: 'daily', 1: 'party', 2: 'traditional', 3: 'wedding', 4: 'dummy'}, 'attr_6': {0: 'default', 1: 'jacquard', 2: 'tassels and latkans', 3: 'dummy'}, 'attr_7': {0: 'default', 1: 'same as saree', 2: 'woven design', 3: 'zari woven', 4: 'dummy'}, 'attr_8': {0: 'default', 1: 'printed', 2: 'solid', 3: 'woven design', 4: 'zari woven', 5: 'dummy'}, 'attr_9': {0: 'applique', 1: 'botanical', 2: 'checked', 3: 'default', 4: 'elephant', 5: 'ethnic motif', 6: 'floral', 7: 'peacock', 8: 'solid', 9: 'dummy'}, 'attr_10': {0: 'no', 1: 'yes', 2: 'dummy'}}\ncategory_path = category.replace(\" \", \"_\")","metadata":{"execution":{"iopub.status.busy":"2024-11-15T05:50:30.568984Z","iopub.execute_input":"2024-11-15T05:50:30.569440Z","iopub.status.idle":"2024-11-15T05:50:30.584579Z","shell.execute_reply.started":"2024-11-15T05:50:30.569398Z","shell.execute_reply":"2024-11-15T05:50:30.583012Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function: Resize images to (384, 384) and normalize pixel values to [-1, 1]\ndef preprocess_image(image_path):\n    img = image.load_img(image_path, target_size=(384, 384))  # Resize to model's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0 \n    img_array = img_array * 2 - 1  # Normalize pixel values to range [-1, 1]\n    return img_array\n\n# Function to predict attributes for images in batches and save to CSV\ndef predict_attributes_and_save_to_csv(image_folder, model, batch_size=128, category=category, num_attribute=num_attribute):\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sort by numerical part of the file name\n\n    # CSV header\n    header = ['id', 'Category', 'len'] + [f'attr_{i+1}' for i in range(10)]\n\n    # Open CSV file\n    with open('v3s.csv', mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=header)\n        writer.writeheader()\n\n        for i in range(0, len(image_files), batch_size):\n            batch_files = image_files[i:i + batch_size]\n            batch_images = np.array([preprocess_image(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Predict attributes for the batch\n            raw_predictions = model.predict(batch_images)\n\n            # Remove the last element from each prediction in raw_predictions\n            raw_predictions = [pred[:, :-1] for pred in raw_predictions]\n\n            # Extract predicted classes for each attribute\n            predicted_classes = [np.argmax(pred, axis=-1) for pred in raw_predictions]\n\n            for idx, file in enumerate(batch_files):\n                # Extract image id by removing leading zeros and file extension\n                image_id = str(int(file.split('.')[0])) if file != '000000.jpg' else '0'\n\n                # Map predicted classes to attribute labels up to num_attribute\n                predicted_labels = {\n                    f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]]\n                    for j in range(num_attribute)\n                }\n\n                # Prepare a row with dynamic attributes and dummy values for the rest\n                row = {\n                    'id': image_id,\n                    'Category': category,\n                    'len': num_attribute\n                }\n                for j in range(1, 11):\n                    attr_key = f'attr_{j}'\n                    row[attr_key] = predicted_labels.get(attr_key, 'dummy')\n\n                # Write the row to the CSV\n                writer.writerow(row)\n\n# Example usage\nimage_folder = f\"/kaggle/input/sorted-test/sorted_images/{category_path}\"  # Update with your folder path\npredict_attributes_and_save_to_csv(image_folder, model, batch_size=128, category=category, num_attribute=num_attribute)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T05:50:36.687982Z","iopub.execute_input":"2024-11-15T05:50:36.688811Z","iopub.status.idle":"2024-11-15T05:54:11.256162Z","shell.execute_reply.started":"2024-11-15T05:50:36.688771Z","shell.execute_reply":"2024-11-15T05:54:11.255326Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1731649844.269371     113 service.cc:145] XLA service 0x793644003350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1731649844.269437     113 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1731649844.269441     113 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 17s/step","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1731649856.589801     113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 207ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 222ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 222ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 230ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 227ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 231ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 236ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 241ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 233ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 237ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 234ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step\n\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17s/step\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function: Resize images to (384, 384) and normalize pixel values to [-1, 1]\ndef preprocess_image(image_path):\n    img = image.load_img(image_path, target_size=(384, 384))  # Resize to model's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0 \n    img_array = img_array * 2 - 1  # Normalize pixel values to range [-1, 1]\n    return img_array\n\n# Function to predict attributes for images in batches and save to CSV\ndef predict_attributes_and_save_to_csv(image_folder, model1, model2, batch_size=128, category=category, num_attribute=num_attribute):\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sort by numerical part of the file name\n\n    # CSV header\n    header = ['id', 'Category', 'len'] + [f'attr_{i+1}' for i in range(10)]\n\n    # Open CSV file\n    with open('inc$eff.csv', mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=header)\n        writer.writeheader()\n\n        for i in range(0, len(image_files), batch_size):\n            batch_files = image_files[i:i + batch_size]\n            batch_images = np.array([preprocess_image(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Predict attributes for the batch with both models\n            raw_predictions1 = model1.predict(batch_images)\n            raw_predictions2 = model2.predict(batch_images)\n\n            # Average the predictions from both models\n            averaged_predictions = [(pred1 + pred2) / 2 for pred1, pred2 in zip(raw_predictions1, raw_predictions2)]\n\n            # Remove the last element from each prediction in averaged_predictions\n            averaged_predictions = [pred[:, :-1] for pred in averaged_predictions]\n\n            # Extract predicted classes for each attribute\n            predicted_classes = [np.argmax(pred, axis=-1) for pred in averaged_predictions]\n\n            for idx, file in enumerate(batch_files):\n                # Extract image id by removing leading zeros and file extension\n                image_id = str(int(file.split('.')[0])) if file != '000000.jpg' else '0'\n\n                # Map predicted classes to attribute labels up to num_attribute\n                predicted_labels = {\n                    f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]]\n                    for j in range(num_attribute)\n                }\n\n                # Prepare a row with dynamic attributes and dummy values for the rest\n                row = {\n                    'id': image_id,\n                    'Category': category,\n                    'len': num_attribute\n                }\n                for j in range(1, 11):\n                    attr_key = f'attr_{j}'\n                    row[attr_key] = predicted_labels.get(attr_key, 'dummy')\n\n                # Write the row to the CSV\n                writer.writerow(row)\n\n# Example usage\nimage_folder = f\"/kaggle/working/sorted_images/{category_path}\"  # Update with your folder path\npredict_attributes_and_save_to_csv(image_folder, model1, model2, batch_size=128, category=category, num_attribute=num_attribute)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T16:13:09.565070Z","iopub.execute_input":"2024-11-05T16:13:09.565471Z","iopub.status.idle":"2024-11-05T16:13:19.782899Z","shell.execute_reply.started":"2024-11-05T16:13:09.565433Z","shell.execute_reply":"2024-11-05T16:13:19.781587Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 195ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     68\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/sorted_images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update with your folder path\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mpredict_attributes_and_save_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_attribute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_attribute\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[16], line 33\u001b[0m, in \u001b[0;36mpredict_attributes_and_save_to_csv\u001b[0;34m(image_folder, model1, model2, batch_size, category, num_attribute)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Predict attributes for the batch with both models\u001b[39;00m\n\u001b[1;32m     32\u001b[0m raw_predictions1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mpredict(batch_images)\n\u001b[0;32m---> 33\u001b[0m raw_predictions2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Average the predictions from both models\u001b[39;00m\n\u001b[1;32m     36\u001b[0m averaged_predictions \u001b[38;5;241m=\u001b[39m [(pred1 \u001b[38;5;241m+\u001b[39m pred2) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred1, pred2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(raw_predictions1, raw_predictions2)]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"inception_multioutput\" is incompatible with the layer: expected shape=(None, 299, 299, 3), found shape=(32, 384, 384, 3)"],"ename":"ValueError","evalue":"Input 0 of layer \"inception_multioutput\" is incompatible with the layer: expected shape=(None, 299, 299, 3), found shape=(32, 384, 384, 3)","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function for model1 and model2: Resize to (384, 384) and normalize to [-1, 1]\ndef preprocess_image_model1_2(image_path):\n    img = image.load_img(image_path, target_size=(384, 384))  # Resize to model's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0\n    img_array = img_array * 2 - 1  # Normalize pixel values to range [-1, 1]\n    return img_array\n\n# Preprocessing function for model3: Resize to (299, 299) and normalize to [0, 1]\ndef preprocess_image_model3(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))  # Resize to 299x299 for model3\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0  # Normalize to range [0, 1] for model3\n    return img_array\n\n# Function to predict attributes for images in batches and save to CSV\ndef predict_attributes_and_save_to_csv(image_folder, model1, model2, model3, batch_size=128, category=category, num_attribute=num_attribute):\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sort by numerical part of the file name\n\n    # CSV header\n    header = ['id', 'Category', 'len'] + [f'attr_{i+1}' for i in range(10)]\n\n    # Open CSV file\n    with open('ensemble3preds.csv', mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=header)\n        writer.writeheader()\n\n        for i in range(0, len(image_files), batch_size):\n            batch_files = image_files[i:i + batch_size]\n\n            # Preprocess images for model1 and model2 (384x384, [-1, 1])\n            batch_images_model1_2 = np.array([preprocess_image_model1_2(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Preprocess images for model3 (299x299, [0, 1])\n            batch_images_model3 = np.array([preprocess_image_model3(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Predict attributes for the batch with all three models\n            raw_predictions1 = model1.predict(batch_images_model1_2)\n            raw_predictions2 = model2.predict(batch_images_model1_2)\n            raw_predictions3 = model3.predict(batch_images_model3)\n\n            # Average the predictions from all three models\n            averaged_predictions = [(pred1 + pred2 + pred3) / 3 for pred1, pred2, pred3 in zip(raw_predictions1, raw_predictions2, raw_predictions3)]\n\n            # Remove the last element from each prediction in averaged_predictions\n            averaged_predictions = [pred[:, :-1] for pred in averaged_predictions]\n\n            # Extract predicted classes for each attribute\n            predicted_classes = [np.argmax(pred, axis=-1) for pred in averaged_predictions]\n\n            for idx, file in enumerate(batch_files):\n                # Extract image id by removing leading zeros and file extension\n                image_id = str(int(file.split('.')[0])) if file != '000000.jpg' else '0'\n\n                # Map predicted classes to attribute labels up to num_attribute\n                predicted_labels = {\n                    f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]]\n                    for j in range(num_attribute)\n                }\n\n                # Prepare a row with dynamic attributes and dummy values for the rest\n                row = {\n                    'id': image_id,\n                    'Category': category,\n                    'len': num_attribute\n                }\n                for j in range(1, 11):\n                    attr_key = f'attr_{j}'\n                    row[attr_key] = predicted_labels.get(attr_key, 'dummy')\n\n                # Write the row to the CSV\n                writer.writerow(row)\n\n# Example usage\nimage_folder = f\"/kaggle/working/sorted_images/{category_path}\"  # Update with your folder path\npredict_attributes_and_save_to_csv(image_folder, model1, model2, model3, batch_size=128, category=category, num_attribute=num_attribute)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T16:01:29.282777Z","iopub.execute_input":"2024-11-05T16:01:29.283676Z","iopub.status.idle":"2024-11-05T16:04:30.977155Z","shell.execute_reply.started":"2024-11-05T16:01:29.283631Z","shell.execute_reply":"2024-11-05T16:04:30.976353Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 213ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step  \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step  \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8s/step \n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function for model1: Resize to (384, 384) and normalize to [-1, 1]\ndef preprocess_image_model1(image_path):\n    img = image.load_img(image_path, target_size=(384, 384))  # Resize to model1's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0\n    img_array = img_array * 2 - 1  # Normalize pixel values to range [-1, 1]\n    return img_array\n\n# Preprocessing function for model3: Resize to (299, 299) and normalize to [0, 1]\ndef preprocess_image_model3(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))  # Resize to 299x299 for model3\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0  # Normalize to range [0, 1] for model3\n    return img_array\n\n# Function to predict attributes for images in batches and save to CSV\ndef predict_attributes_and_save_to_csv(image_folder, model1, model3, batch_size=128, category=category, num_attribute=num_attribute):\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sort by numerical part of the file name\n\n    # CSV header\n    header = ['id', 'Category', 'len'] + [f'attr_{i+1}' for i in range(10)]\n\n    # Open CSV file\n    with open('inc$eff_1.csv', mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=header)\n        writer.writeheader()\n\n        for i in range(0, len(image_files), batch_size):\n            batch_files = image_files[i:i + batch_size]\n\n            # Preprocess images for model1 (384x384, [-1, 1])\n            batch_images_model1 = np.array([preprocess_image_model1(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Preprocess images for model3 (299x299, [0, 1])\n            batch_images_model3 = np.array([preprocess_image_model3(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Predict attributes for the batch with both models\n            raw_predictions1 = model1.predict(batch_images_model1)\n            raw_predictions3 = model3.predict(batch_images_model3)\n\n            # Average the predictions from model1 and model3\n            averaged_predictions = [(pred1 + pred3) / 2 for pred1, pred3 in zip(raw_predictions1, raw_predictions3)]\n\n            # Remove the last element from each prediction in averaged_predictions\n            averaged_predictions = [pred[:, :-1] for pred in averaged_predictions]\n\n            # Extract predicted classes for each attribute\n            predicted_classes = [np.argmax(pred, axis=-1) for pred in averaged_predictions]\n\n            for idx, file in enumerate(batch_files):\n                # Extract image id by removing leading zeros and file extension\n                image_id = str(int(file.split('.')[0])) if file != '000000.jpg' else '0'\n\n                # Map predicted classes to attribute labels up to num_attribute\n                predicted_labels = {\n                    f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]]\n                    for j in range(num_attribute)\n                }\n\n                # Prepare a row with dynamic attributes and dummy values for the rest\n                row = {\n                    'id': image_id,\n                    'Category': category,\n                    'len': num_attribute\n                }\n                for j in range(1, 11):\n                    attr_key = f'attr_{j}'\n                    row[attr_key] = predicted_labels.get(attr_key, 'dummy')\n\n                # Write the row to the CSV\n                writer.writerow(row)\n\n# Example usage\nimage_folder = f\"/kaggle/working/sorted_images/{category_path}\"  # Update with your folder path\npredict_attributes_and_save_to_csv(image_folder, model1, model3, batch_size=128, category=category, num_attribute=num_attribute)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T16:16:22.054240Z","iopub.execute_input":"2024-11-05T16:16:22.054678Z","iopub.status.idle":"2024-11-05T16:18:18.372394Z","shell.execute_reply.started":"2024-11-05T16:16:22.054639Z","shell.execute_reply":"2024-11-05T16:18:18.371455Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 252ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step  \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport csv\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Preprocessing function for model1: Resize to (384, 384) and normalize to [-1, 1]\ndef preprocess_image_model1(image_path):\n    img = image.load_img(image_path, target_size=(384, 384))  # Resize to model1's input size\n    img_array = image.img_to_array(img)                       # Convert to array\n    img_array = img_array / 255.0\n    img_array = img_array * 2 - 1  # Normalize pixel values to range [-1, 1]\n    return img_array\n\n# Preprocessing function for model2: Resize to (384, 384) and normalize to [-1, 1]\ndef preprocess_image_model2(image_path):\n    img = image.load_img(image_path, target_size=(384, 384))  # Resize to model2's input size\n    img_array = image.img_to_array(img)\n    img_array = img_array / 255.0\n    img_array = img_array * 2 - 1  # Normalize pixel values to range [-1, 1]\n    return img_array\n\n# Preprocessing function for model3: Resize to (299, 299) and normalize to [0, 1]\ndef preprocess_image_model3(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))  # Resize to 299x299 for model3\n    img_array = image.img_to_array(img)\n    img_array = img_array / 255.0  # Normalize to range [0, 1] for model3\n    return img_array\n\n# Function to predict attributes for images in batches and save to CSV\ndef predict_attributes_and_save_to_csv(image_folder, model1, model2, model3, batch_size=128, category=category, num_attribute=num_attribute):\n    image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')],\n                         key=lambda x: int(x.split('.')[0]))  # Sort by numerical part of the file name\n\n    # CSV header\n    header = ['id', 'Category', 'len'] + [f'attr_{i+1}' for i in range(10)]\n\n    # Open CSV file\n    with open('highest.csv', mode='w', newline='') as file:\n        writer = csv.DictWriter(file, fieldnames=header)\n        writer.writeheader()\n\n        for i in range(0, len(image_files), batch_size):\n            batch_files = image_files[i:i + batch_size]\n\n            # Preprocess images for each model\n            batch_images_model1 = np.array([preprocess_image_model1(os.path.join(image_folder, file)) for file in batch_files])\n            batch_images_model2 = np.array([preprocess_image_model2(os.path.join(image_folder, file)) for file in batch_files])\n            batch_images_model3 = np.array([preprocess_image_model3(os.path.join(image_folder, file)) for file in batch_files])\n\n            # Predict attributes for the batch with all three models\n            raw_predictions1 = model1.predict(batch_images_model1)\n            raw_predictions2 = model2.predict(batch_images_model2)\n            raw_predictions3 = model3.predict(batch_images_model3)\n\n            # Take the maximum value across predictions from all three models for each attribute\n            combined_predictions = [np.maximum(np.maximum(pred1, pred2), pred3) \n                                    for pred1, pred2, pred3 in zip(raw_predictions1, raw_predictions2, raw_predictions3)]\n\n            # Remove the last element from each prediction in combined_predictions\n            combined_predictions = [pred[:, :-1] for pred in combined_predictions]\n\n            # Extract predicted classes for each attribute\n            predicted_classes = [np.argmax(pred, axis=-1) for pred in combined_predictions]\n\n            for idx, file in enumerate(batch_files):\n                # Extract image id by removing leading zeros and file extension\n                image_id = str(int(file.split('.')[0])) if file != '000000.jpg' else '0'\n\n                # Map predicted classes to attribute labels up to num_attribute\n                predicted_labels = {\n                    f'attr_{j + 1}': attribute_mapping[f'attr_{j + 1}'][predicted_classes[j][idx]]\n                    for j in range(num_attribute)\n                }\n\n                # Prepare a row with dynamic attributes and dummy values for the rest\n                row = {\n                    'id': image_id,\n                    'Category': category,\n                    'len': num_attribute\n                }\n                for j in range(1, 11):\n                    attr_key = f'attr_{j}'\n                    row[attr_key] = predicted_labels.get(attr_key, 'dummy')\n\n                # Write the row to the CSV\n                writer.writerow(row)\n\n# Example usage\nimage_folder = f\"/kaggle/working/sorted_images/{category_path}\"  # Update with your folder path\npredict_attributes_and_save_to_csv(image_folder, model1, model2, model3, batch_size=128, category=category, num_attribute=num_attribute)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-05T16:33:18.919890Z","iopub.execute_input":"2024-11-05T16:33:18.920544Z","iopub.status.idle":"2024-11-05T16:36:52.631199Z","shell.execute_reply.started":"2024-11-05T16:33:18.920503Z","shell.execute_reply":"2024-11-05T16:36:52.630422Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 208ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 237ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 192ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 245ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 250ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 248ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 211ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step\n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step  \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4s/step  \n\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport glob\n\n# List of CSV file paths\ncsv_files = ['/kaggle/working/v3m.csv', '/kaggle/working/v3s.csv', '/kaggle/working/v3k.csv', '/kaggle/working/v3w.csv', '/kaggle/working/v3wt.csv']  # Replace with actual file paths\n\n# Read all CSV files into a single DataFrame\ndf_list = [pd.read_csv(file) for file in csv_files]\n\n# Concatenate all DataFrames into one\ncombined_df = pd.concat(df_list, ignore_index=True)\n\n# Convert 'id' column to numeric for proper sorting\ncombined_df['id'] = pd.to_numeric(combined_df['id'], errors='coerce')\n\n# Sort the DataFrame by 'id' column in ascending order\nsorted_df = combined_df.sort_values(by='id')\n\n# Write the sorted DataFrame to a new CSV file\nsorted_df.to_csv('bettercorr.csv', index=False)\n\nprint(\"Merged and sorted CSV file created successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T05:40:37.983307Z","iopub.execute_input":"2024-11-15T05:40:37.983767Z","iopub.status.idle":"2024-11-15T05:40:38.342042Z","shell.execute_reply.started":"2024-11-15T05:40:37.983727Z","shell.execute_reply":"2024-11-15T05:40:38.341031Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Merged and sorted CSV file created successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the two CSV files\ncsv1_path = '/kaggle/input/only-dummy/only_dummy.csv'\ncsv2_path = '/kaggle/working/v3s.csv'\n\ndf1 = pd.read_csv(csv1_path)\ndf2 = pd.read_csv(csv2_path)\n\n# Merge DataFrames by 'id', prioritizing rows from df2 when ids match\ndf_merged = pd.merge(df1, df2, on='id', how='left', suffixes=('', '_new'))\n\n# Update columns in df1 with the values from df2 wherever the 'id' matched\nfor col in df2.columns:\n    if col != 'id':  # Avoid updating the 'id' column itself\n        df_merged[col] = df_merged[col + '_new'].combine_first(df_merged[col])\n        df_merged.drop(columns=[col + '_new'], inplace=True)\n\n# Save the result to a new CSV file\ndf_merged.to_csv('betttercorr_s.csv', index=False)\n\nprint(\"CSV updated successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T05:54:53.306914Z","iopub.execute_input":"2024-11-15T05:54:53.307578Z","iopub.status.idle":"2024-11-15T05:54:53.741751Z","shell.execute_reply.started":"2024-11-15T05:54:53.307530Z","shell.execute_reply":"2024-11-15T05:54:53.740772Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CSV updated successfully.\n","output_type":"stream"}]}]}